"""
VulnerabilitySpotter Module

Multi-metric risk aggregation detecting vulnerabilities via scarcity, entropy,
and deceptive variance with Bayesian log-odds or weighted sum aggregation.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class VulnerabilitySpotter(nn.Module):
    """
    Multi-metric risk aggregation (scarcity, entropy, deceptive variance) with Bayesian log-odds or weighted sum.
    Returns: v_t (batch), all sub-metrics in metadata.
    """
    def __init__(self, d_model=256, aggregation_method='bayesian'):
        super().__init__()
        self.semantic_encoder = nn.Linear(d_model, 128)
        self.scarcity_head = nn.Linear(128, 1)
        self.deceptive_head = nn.Linear(d_model, 1)
        self.entropy_high, self.entropy_low = 3.0, 2.5
        self.aggregation_method = aggregation_method
        self.weighted_sum_weights = nn.Parameter(torch.tensor([0.5, 0.3, 0.2], dtype=torch.float32))
        self.epsilon = 1e-8

        nn.init.xavier_uniform_(self.semantic_encoder.weight)
        nn.init.xavier_uniform_(self.scarcity_head.weight)
        nn.init.xavier_uniform_(self.deceptive_head.weight)
        self.scarcity_head.bias.data.fill_(0.5)
        self.deceptive_head.bias.data.fill_(0.5)

    def _shannon_entropy(self, attn_probs):
        """Shannon entropy over seq for each batch (scalar diagnostic for grad risk)."""
        p = attn_probs + self.epsilon
        return -(p * torch.log2(p)).sum(dim=-1)

    def forward(self, x, attention_weights=None, audit_mode=False):
        if x.ndim == 2:
            x = x.unsqueeze(1)
        batch, seq, d_model = x.shape

        # Scarcity: mean semantic stress
        encoded = F.relu(self.semantic_encoder(x.mean(dim=1)))
        scarcity = torch.sigmoid(self.scarcity_head(encoded)).squeeze(-1)

        # Entropy: on attn (or dynamic if not present)
        entropy = torch.zeros(batch, device=x.device)
        entropy_risk = torch.zeros_like(scarcity)
        if attention_weights is not None:
            entropy = self._shannon_entropy(attention_weights.mean(dim=1))
            entropy_risk = ((entropy > self.entropy_high) | (entropy < self.entropy_low)).float() * 0.3
            entropy_risk = torch.clamp(entropy_risk, min=0.01)
        else:
            entropy_risk = torch.rand_like(scarcity) * 0.40 + 0.10

        # Deceptive: variance of hidden states across sequence
        var_hidden = torch.var(x, dim=1)
        deceptive = torch.sigmoid(self.deceptive_head(var_hidden)).squeeze(-1)

        # Apply scaling factors
        scarcity_scaled = scarcity * 1.0
        deceptive_scaled = deceptive * 1.0
        entropy_risk_scaled = entropy_risk * 1.5

        # Aggregate risks based on selected method
        if self.aggregation_method == 'bayesian':
            scarcity_p = torch.clamp(scarcity_scaled, self.epsilon, 1 - self.epsilon)
            entropy_p = torch.clamp(entropy_risk_scaled, self.epsilon, 1 - self.epsilon)
            deceptive_p = torch.clamp(deceptive_scaled, self.epsilon, 1 - self.epsilon)

            log_odds_scarcity = torch.log(scarcity_p / (1 - scarcity_p))
            log_odds_entropy = torch.log(entropy_p / (1 - entropy_p))
            log_odds_deceptive = torch.log(deceptive_p / (1 - deceptive_p))

            aggregated_log_odds = log_odds_scarcity + log_odds_entropy + log_odds_deceptive
            v_t_calibrated_scalar = aggregated_log_odds

        elif self.aggregation_method == 'weighted_sum':
            risks_stacked = torch.stack([scarcity_scaled, entropy_risk_scaled, deceptive_scaled], dim=1)
            weights = self.weighted_sum_weights.to(x.device)
            v_t_calibrated_scalar = (risks_stacked * weights).sum(dim=1)
        else:
            raise ValueError(f"Unknown aggregation method: {self.aggregation_method}")

        if audit_mode:
            print("\n--- VulnerabilitySpotter Diagnostics (Before Aggregation Adjustments) ---")
            print("Scarcity (pre-agg):", scarcity.detach().cpu().numpy())
            print("Entropy_risk (pre-agg):", entropy_risk.detach().cpu().numpy())
            print("Deceptive (pre-agg):", deceptive.detach().cpu().numpy())
            print("----------------------------------------\n")

        v_t_calibrated_tensor = v_t_calibrated_scalar.unsqueeze(-1).unsqueeze(-1).expand(-1, seq, -1)

        metadata = {
            'scarcity': scarcity_scaled.unsqueeze(-1).unsqueeze(-1),
            'entropy': entropy.unsqueeze(-1).unsqueeze(-1),
            'entropy_risk': entropy_risk_scaled.unsqueeze(-1).unsqueeze(-1),
            'deceptive': deceptive_scaled.unsqueeze(-1).unsqueeze(-1),
            'v_t': v_t_calibrated_tensor
        }

        if audit_mode:
            print("\n--- VulnerabilitySpotter Diagnostics (After Aggregation) ---")
            if self.aggregation_method == 'bayesian':
                print("Scarcity (post-agg - prob):", scarcity_p.detach().cpu().numpy())
                print("Entropy_risk (post-agg - prob):", entropy_p.detach().cpu().numpy())
                print("Deceptive (post-agg - prob):", deceptive_p.detach().cpu().numpy())
                print("v_t (post-calibration - log-odds):", v_t_calibrated_scalar.detach().cpu().numpy())
            elif self.aggregation_method == 'weighted_sum':
                print("Risks (post-agg - scaled):", risks_stacked.detach().cpu().numpy())
                print("v_t (post-calibration - weighted_sum):", v_t_calibrated_scalar.detach().cpu().numpy())
            print("----------------------------------------\n")

        return v_t_calibrated_tensor, metadata
